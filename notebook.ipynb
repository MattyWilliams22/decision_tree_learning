{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from typing import Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "SIGNAL_COUNT = 7\n",
    "CLASS_COUNT = 4\n",
    "NUM_FOLFDS = 10\n",
    "\n",
    "clean_dataset = np.loadtxt(\"wifi_db/clean_dataset.txt\", delimiter=\"\\t\")\n",
    "noisy_dataset = np.loadtxt(\"wifi_db/noisy_dataset.txt\", delimiter=\" \")\n",
    "\n",
    "TEST_DATASET = clean_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# decision_tree_learning\n",
    "\n",
    "def all_same_label(dataset) -> bool:\n",
    "    # check if all samples have the same label\n",
    "    for i in range(len(dataset)):\n",
    "        if dataset[i][SIGNAL_COUNT] != dataset[0][SIGNAL_COUNT]:\n",
    "            return False    \n",
    "    return True\n",
    "\n",
    "def entropy(dataset) -> float:\n",
    "    # H(D) = - Sum(P(X) * log2(P(X))) for each X in Classes\n",
    "\n",
    "    if len(dataset) == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    entropy = 0.0\n",
    "    counts = [0] * CLASS_COUNT\n",
    "\n",
    "    for i in range(len(dataset)):\n",
    "        class_index = int(dataset[i][SIGNAL_COUNT]) - 1\n",
    "        counts[class_index] = counts[class_index] + 1\n",
    "\n",
    "    for c in range(CLASS_COUNT):\n",
    "        if counts[c] > 0:\n",
    "            fraction = counts[c] / len(dataset)\n",
    "            entropy -= fraction * np.log2(fraction)\n",
    "\n",
    "    return entropy\n",
    "\n",
    "def calculate_IG(full_dataset, l_dataset, r_dataset) -> float:\n",
    "    # IG(D1, D2) = H(D) - H(D1, D2)\n",
    "    # H(D1, D2) = (|D1|/|D|)H(D1) + (|D2|/|D|)H(D2)\n",
    "\n",
    "    # H(D) and |D|\n",
    "    full_entropy = entropy(full_dataset)\n",
    "    full_length = len(full_dataset)\n",
    "\n",
    "    # (|D1|/|D|) and (|D2|/|D|)\n",
    "    if full_length > 0:\n",
    "        l_fraction = len(l_dataset) / full_length\n",
    "        r_fraction = len(r_dataset) / full_length\n",
    "    else:\n",
    "        l_fraction = 0.0\n",
    "        r_fraction = 0.0\n",
    "\n",
    "    # H(D1) and H(D2)\n",
    "    l_entropy = entropy(l_dataset)\n",
    "    r_entropy = entropy(r_dataset)\n",
    "\n",
    "    # H(D1, D2)\n",
    "    combined_entropy = l_fraction * l_entropy + r_fraction * r_entropy\n",
    "\n",
    "    # return IG(D1, D2)\n",
    "    return full_entropy - combined_entropy\n",
    "\n",
    "def pick_split_value(dataset, attribute: int) -> Tuple[float, float]:\n",
    "    # find the best value to split by\n",
    "    highest_IG = 0.0\n",
    "    best_value = None\n",
    "\n",
    "    dataset = dataset[dataset[:, attribute].argsort()]\n",
    "\n",
    "    prev_value = dataset[0][attribute]\n",
    "\n",
    "    for i in range(1, len(dataset) - 1):\n",
    "        if dataset[i][attribute] == prev_value:\n",
    "            continue\n",
    "        else: \n",
    "            prev_value = dataset[i][attribute]\n",
    "        \n",
    "        l_dataset = dataset[:i]\n",
    "        r_dataset = dataset[i:]\n",
    "\n",
    "        information_gain = calculate_IG(dataset, l_dataset, r_dataset)\n",
    "\n",
    "        if information_gain > highest_IG:\n",
    "            highest_IG = information_gain\n",
    "            best_value = dataset[i][attribute]\n",
    "    \n",
    "    return (highest_IG, best_value)\n",
    "\n",
    "def find_split(dataset):\n",
    "    # find the best split\n",
    "\n",
    "    if len(dataset) == 2:\n",
    "        i = 0\n",
    "        while dataset[0][i] == dataset[1][i]:\n",
    "            i = i + 1\n",
    "        print(\"Splitting attribute \" + str(i) + \" at value \" + str(max(dataset[0][i], dataset[1][i])))\n",
    "        return (i, max(dataset[0][i], dataset[1][i]))\n",
    "    \n",
    "    best_attribute = 0\n",
    "    best_value = None\n",
    "    best_IG = 0.0\n",
    "\n",
    "    for i in range(SIGNAL_COUNT):\n",
    "        (information_gain, value) = pick_split_value(dataset, i)\n",
    "\n",
    "        if value is not None and information_gain > best_IG:\n",
    "            best_IG = information_gain\n",
    "            best_attribute = i\n",
    "            best_value = value\n",
    "\n",
    "    if best_value is None:\n",
    "        best_value = dataset[1][best_attribute]\n",
    "    \n",
    "    print(\"Splitting attribute \" + str(best_attribute) + \" at value \" + str(best_value))\n",
    "    return (best_attribute, best_value)\n",
    "\n",
    "def decision_tree_learning(dataset, depth) -> Tuple[dict, int]:\n",
    "    print(\"Depth: \" + str(depth))\n",
    "    if depth > 50:\n",
    "        return ({\"error\": \"error\"}, depth)\n",
    "    if all_same_label(dataset):\n",
    "        print(\"Value: \" + str(dataset[0][SIGNAL_COUNT]))\n",
    "        return ({\"value\": dataset[0][SIGNAL_COUNT]}, depth)\n",
    "    else:\n",
    "        (split_attribute, split_value) = find_split(dataset)\n",
    "\n",
    "        l_dataset = dataset[dataset[:,split_attribute] < split_value]\n",
    "        r_dataset = dataset[dataset[:,split_attribute] >= split_value]\n",
    "\n",
    "        node = {\"split_attribute\": split_attribute, \"split_value\": split_value}\n",
    "\n",
    "        l_branch, l_depth = decision_tree_learning(l_dataset, depth+1)\n",
    "        r_branch, r_depth = decision_tree_learning(r_dataset, depth+1)\n",
    "\n",
    "        node[\"l_branch\"] = l_branch\n",
    "        node[\"r_branch\"] = r_branch\n",
    "\n",
    "        return (node, max(l_depth, r_depth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = decision_tree_learning(TEST_DATASET, 0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tree visualiser\n",
    "\n",
    "# adjust size here later to make it fit depending on the tree depth\n",
    "fig_height = 2 ** 5\n",
    "fig_width = 2 ** 6\n",
    "fig, ax = plt.subplots(figsize=(fig_width, fig_height)) \n",
    "ax.set_xlim(0, fig_width)\n",
    "ax.set_ylim(0, fig_height)\n",
    "ax.axis(\"off\")\n",
    "\n",
    "def plot_node(node, x, y, child_offset, line_color):\n",
    "    if line_color == \"blue\":\n",
    "        line_color = \"red\"\n",
    "    else:\n",
    "        line_color = \"blue\"\n",
    "    if \"value\" in node:\n",
    "        ax.annotate(f\"leaf: {node[\"value\"]}\", (x, y), fontsize=12, ha=\"center\", va=\"center\", bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"blue\"))\n",
    "    else:\n",
    "        ax.annotate(f\"[x{node['split_attribute']} < {node['split_value']}]\", (x, y), fontsize=12, ha=\"center\", va=\"center\", bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"blue\"))\n",
    "        plot_node(node[\"l_branch\"], x-child_offset, y-2, child_offset//2, line_color)\n",
    "        plot_node(node[\"r_branch\"], x+child_offset, y-2, child_offset//2, line_color)\n",
    "        ax.add_line(plt.Line2D((x, x-child_offset), (y, y-2), c=line_color))\n",
    "        ax.add_line(plt.Line2D((x, x+child_offset), (y, y-2), c=line_color))\n",
    "\n",
    "demo_tree = {\"split_attribute\": 1, \"split_value\": 5, \"l_branch\": {\"value\": 1}, \"r_branch\": {\"split_attribute\": 2, \"split_value\": 3, \"l_branch\": {\"value\": 2}, \"r_branch\": {\"value\": 3}}}\n",
    "plot_node(tree, fig_width//2, fig_height, fig_width//4,\"blue\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics\n",
    "\n",
    "def confusion(y_actual, y_prediction, class_labels):\n",
    "\n",
    "    confusion = np.zeros((len(class_labels), len(class_labels)), dtype=np.int64)\n",
    "\n",
    "    for i, label in enumerate(class_labels):\n",
    "        indices = y_actual == label\n",
    "        predictions = y_prediction[indices]\n",
    "\n",
    "        (unique_labels, counts) = np.unique(predictions, return_counts=True)\n",
    "\n",
    "        frequency_dict = dict(zip(unique_labels, counts))\n",
    "\n",
    "        for j, class_label in enumerate(class_labels):\n",
    "            confusion[i, j] = frequency_dict.get(class_label, 0)\n",
    "\n",
    "    return confusion\n",
    "\n",
    "def accuracy(confusion):\n",
    "    if np.sum(confusion) > 0:\n",
    "        return np.sum(np.diag(confusion)) / np.sum(confusion)\n",
    "    return 0.0\n",
    "\n",
    "def precision(confusion):\n",
    "    precision = np.zeros((len(confusion),))\n",
    "    for c in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[:, c]) > 0:\n",
    "            precision[c] = confusion[c, c] / np.sum(confusion[:, c])\n",
    "\n",
    "    return precision\n",
    "\n",
    "def recall(confusion):\n",
    "    recall = np.zeros((len(confusion),))\n",
    "    for col in range(confusion.shape[0]):\n",
    "        if np.sum(confusion[col, :]) > 0:\n",
    "            recall[col] = confusion[col, col] / np.sum(confusion[col, :])\n",
    "\n",
    "    return recall\n",
    "\n",
    "def f1_score(precision, recall):\n",
    "    f1 = np.zeros((len(precision),))\n",
    "    for c, (p, r) in enumerate(zip(precision, recall)):\n",
    "        if p + r > 0:\n",
    "            f1[c] = 2 * p * r / (p + r)\n",
    "\n",
    "    return f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "\n",
    "def is_leaf(tree):\n",
    "\treturn not \"l_branch\" in tree\n",
    "\n",
    "def get_label(tree, values):\n",
    "\tif is_leaf(tree):\n",
    "\t\treturn tree[\"value\"]\n",
    "\ttraverse_direction = (\n",
    "\t\t\"l_branch\" if values[tree[\"split_attribute\"]] < tree[\"split_value\"] else \"r_branch\"\n",
    "\t)\n",
    "\treturn get_label(tree[traverse_direction], values)\n",
    "\n",
    "def evaluate(test_db, tree):\n",
    "\ty_actual = np.array(test_db[:, -1])\n",
    "\tif len(y_actual) == 0:\n",
    "\t\treturn 0.0\n",
    "\ty_prediction = np.array([get_label(tree, row) for row in test_db])\n",
    "\treturn np.sum(y_actual == y_prediction) / len(y_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(evaluate(clean_dataset, tree))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision_trees",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
